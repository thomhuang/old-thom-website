---
layout: post
title: Blog Post 2
permalink: /projects/instructional_blogs/blog-post-two/
---
## Developing a Spectral Cluster Algorithm

{::options parse_block_html="true" /}
<div class="got-help">
Made changes from general help that I got from the peer-reviews with regard to:

* trying to avoid using the explanations Professor Chodrow gave us in the outline
* explaining some parameters/removing some that I don't understand quite as well
* making sure the figures are correct to the explanations and code

Throughout the blog post. There are multiple instances in which I change these things, so it'd be a bit hard to put a gave-help section for each one, so I thought I'd do an overarching one to explain the overall changes :)
</div>
{::options parse_block_html="false" /}

In this blog post today, we will be discussing a simple version of a spectral clustering algorithm for clustering data points. It's an important tool for identifying meaningful parts of data sets with complex structure. 

In the two plots below, we will see that there are clear distinctions between the two clusters of data. 

If we use `K-means` from `sklearn`, we can easily separate the clusters by color of the first plot as they are nicely separated into distinct "bubbles," but things become a bit finicky with the second plot's overlapping crescents.

Thus, this arises the notion of the spectral clustering algorithm we are going to be developing today to be able to separate our clusters, no matter how irregular the clusters are in data.

We will be using the following packages for this post today, most of which you should be familiar with:


```python
import numpy as np
from sklearn import datasets
from matplotlib import pyplot as plt
```

And below are the plots mentioned above:


```python
np.random.seed(1111) # Sets seed for consistency in testing/user testing

n = 200 # Arbitrarily chose 200 data points 

X, y = datasets.make_blobs(n_samples=n, shuffle=True, random_state=None, centers = 2, cluster_std = 2.0) # Creates our two "blobs"
plt.scatter(X[:,0], X[:,1]) # Plots our data points
```




    <matplotlib.collections.PathCollection at 0x1f9c243f7f0>




    
![blog_post_2_plot1.png](/images/blog_post_2_plot1.png)
    


We can see two clear "blobs" in their own sectors of the plot essentially, and below we can observe that `KMeans()` will distinctly color them:


```python
from sklearn.cluster import KMeans # Our initial means of separating clusters

km = KMeans(n_clusters = 2) # We use K-means to separate our data into two clusters
km.fit(X) # ACompute our k-means clustering on X

plt.scatter(X[:,0], X[:,1], c = km.predict(X)) # Compute cluster centers and predict cluster index for each sample.
```




    <matplotlib.collections.PathCollection at 0x1f9c2cdc400>




    
![blog_post_2_plot2.png](/images/blog_post_2_plot2.png)
    


The same idea for our blobs apply for the crescents:


```python
np.random.seed(1234) 

n = 200 

X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None) # Rather than "blobs" we create "moons"
plt.scatter(X[:,0], X[:,1])
```




    <matplotlib.collections.PathCollection at 0x1f9c2d36160>




    
![blog_post_2_plot3.png](/images/blog_post_2_plot3.png)
    


Although we can observe that there are two separate moons, they aren't exactly in their own distinct areas of the plot, causing our cluster separating to not work as desired:


```python
from sklearn.cluster import KMeans

km = KMeans(n_clusters = 2)
km.fit(X)

plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```




    <matplotlib.collections.PathCollection at 0x1f9c2d92160>




    
![blog_post_2_plot4.png](/images/blog_post_2_plot4.png)
    


## 1. Constructing our Similarity Matrix and the Norm Cut Objective

Our first step is to create an `(n x n)` similarity matrix $$\mathbf{A}$$ where `n` represents the number of data points we are working. We use a parameter called `epsilon` ($$\epsilon$$) to declare the distance allowed between values in our clusters in the construction of the similarity matrix. 

To explain, `epsilon` dictates if we make some matrix entry $$A_{i,j}$$ is $$1$$. If our data point $$X[i]$$ is within a distance of our `epsilon` of another data point $$X[j]$$, we make the entry $$A_{i,j} = 1$$. Lastly, all diagonal entries of our similarity matrix is $$0$$ of course as our distance between a data point to itself is always going to $$0$$. i.e. $$A_{i,i} = 0$$ for $$1 \leq i \leq n$$.

For the data we'll be using in this blog post (the most recent crescents above), we will use an `espilon` value of $$0.4$$. 

In order to calculate our similarity matrix associated to our `epsilon` value, we could use a for- loop by testing whether

$$(X[i] - X[j])^{2} < \epsilon^{2}$$ 

for each choice of `i` and `j`, however as we have larger and larger data sets, it will become wildly inefficient. So, we will use `sklearn`'s `pairwise_distances` to calculate the distance between our data points in an efficient manner:


```python
# create a pairwise euclidean distance matrix using a sklearn build-in function
from sklearn.metrics import pairwise_distances

epsilon = 0.4 # Defining our epsilon

P = pairwise_distances(X, metric = 'euclidean') # Our distance matrix from data set X

A = np.ndarray(shape = (n,n)) # Creates an empty n x n matrix for us to populate
A[P < epsilon**2] = 1 # If our distance matrix value is within our chosen epsilon, make entry 1
A[P >= epsilon**2] = 0 # If our distance matrix value is greater than our chosen epsilon, make entry 0

np.fill_diagonal(A, 0) # Makes sure our diagonal entries are 0

A
```




    array([[0., 0., 0., ..., 0., 0., 0.],
           [0., 0., 0., ..., 0., 0., 0.],
           [0., 0., 0., ..., 0., 0., 0.],
           ...,
           [0., 0., 0., ..., 0., 0., 1.],
           [0., 0., 0., ..., 0., 0., 0.],
           [0., 0., 0., ..., 1., 0., 0.]])


To explain our `euclidean` metric, it the gives a "standard" distance between any two vectors in $$\mathbb{R}^{n}$$, and in our case, $$\mathbb{R}^{2}$$, where it essentially calculates the distance formula for us between points in our data in a more efficient manner than simply using a for-loop.

{::options parse_block_html="true" /}
<div class="gave-help">
For one of the student's blog post that I reviewed, they had used a for-loop to construct their similarity matrix, such as:

```python
    for col in range(A.shape[1]):
        A[:,col][A[:,col] < epsilon**2] = 1
        A[:,col][A[:,col] != 1] = 0

    np.fill_diagonal(A,0) # make all diagonal entries 0
```

whereas I recommended a more clean, efficient, and intuitive/understandable method for them to go about it by using masking with our epsilon value:

```python
A = np.ndarray(shape = (n,n)) 
A[P < epsilon**2] = 1 
A[P >= epsilon**2] = 0 

np.fill_diagonal(A, 0)
```

Where we construct an empty array, and change values pending their distance from $$\epsilon^{2}$$ instead of looping through the array
</div>
{::options parse_block_html="false" /}

{::options parse_block_html="true" /}
<div class="got-help">

Although I gave a recommendation to a student to make theirs more efficient and concise, I also got help in the same way for constructing the similarity matrix! One student said I could have made the process of making the similarity matrix using a function, as well as making the code more concise than I thought I could have! 

So going from:

```python
A = np.ndarray(shape = (n,n)) 
A[P < epsilon**2] = 1 
A[P >= epsilon**2] = 0 

np.fill_diagonal(A, 0) 
```

to doing this instead:

```python
def construct_similarity(X, epsilon):
    
    P = pairwise_distances(X, metric = 'euclidean')
    
    A = np.zeros(shape = (n,n))
    A[P < epsilon**2] = 1
    np.fill_diagonal(A, 0)

    return A
```

Where we construct the array of 0's, and make every value not in within our epsilon distance as value 1, and everything else should be 0!

As we're already here, I explained the metric used for our pairwise_distance function with its purpose and mathematical explanation :)
</div>
{::options parse_block_html="false" /}

Let us first define some things before delving into the Binary Norm Cut Objective.

* $$d_i = \sum_{j = 1}^n a_{ij}$$ is the $$i$$th row-sum of $$\mathbf{A}$$ (the degree of $$i$$)

* $$C_i$$ is the $$i$$'th cluster within our data. In our case we have $$i \in 0 , 1$$ for two clusters
    * We assume that all of our data points $$x \in C_{i}$$ for all $$i$$
    
* We think of `y[i]` as being the label of point `i`. So, if `y[i] = 1`, then point `i` (and therefore row $$i$$ of $$\mathbf{A}$$) is an element of cluster $$C_1$$ for example.  

Now we are ready to move onto the idea of the Norm Cut Objective. Our new matrix $$\mathbf{A}$$ now informs us of what points are within a distance $$\epsilon$$ from other data points, and now we must cluster our data points by esentially splitting up the rows and columns of our similarity matrix.

The *binary norm cut objective* of a matrix $$\mathbf{A}$$ is the function 

$$N_{\mathbf{A}}(C_0, C_1)\equiv \mathbf{cut}(C_0, C_1)\left(\frac{1}{\mathbf{vol}(C_0)} + \frac{1}{\mathbf{vol}(C_1)}\right)\;.$$

In the above,  
- $$\mathbf{cut}(C_0, C_1) \equiv \sum_{i \in C_0, j \in C_1} a_{ij}$$ is the *cut* of the clusters $$C_0$$ and $$C_1$$. 
- $$\mathbf{vol}(C_0) \equiv \sum_{i \in C_0}d_i$$. The *volume* of cluster $$C_0$$ is a measure of the size of the cluster. 

# The Cut Term

$$\mathbf{cut}(C_0, C_1)$$ as mentioned above is the count of the non-zero entries of our matrix constructed above. By saying that this term should be small is essentially saying that our points in one cluster shouldn't be very close to points in another cluster.

So, we will write our function `cut(A ,y)`:


```python
def cut(A , y):
        
    C_0_values = np.where(y == 0)[0] # Values in cluster 0
    C_1_values = np.where(y == 1)[0] # values in cluster 1

    cut_sum = 0
    
    for rows in C_0_values: # Is the summation described when defining cut(C_0, C_1)
        for cols in C_1_values:
            cut_sum += A[rows][cols]
            
    # returns the sum of points between the two arrays associated to C_0 and C_1
    return cut_sum
```

{::options parse_block_html="true" /}
<div class="gave-help">
In one of the blog posts that I assessed, they did a nested for loop and used the following condition of `y[i] != y[j]: cut_sum += A[i,j]` as shown below:

```python
    for i in range(n):
        for j in range(n):
            # Check if i and j belong to different cluster, then add ups entries
            if y[i] != y[j]: cut_sum += A[i,j]
```

However, using masking would be a lot more understandable and more efficient in terms of computing power and conciseness, so I recommended:
```python
#in your cut function, you could make a mask for your clusters, i.e.
    C_0_values = np.where(y == 0)[0] 
    C_1_values = np.where(y == 1)[0]
```

</div>
{::options parse_block_html="false" /}

Using the data from the two crescents above, we will calculate the true cut value versus a random cut value. 

In the below, we should find that the cut objective for the true labels for our data is *much* smaller (in fact it's 0) than the cut objective for some arbitrarily made label. This proves that our cut objective favors the true one rather than some random one.


```python
np.random.seed(1234) # Set the seed for replication purposes

print("True label cut is: ", cut(A, y))

y_random = np.random.randint(2, size = n) # Creates a label, randomly assigning 0s and 1s
print("Arbitrary label cut is: ", cut(A, y_random))
```

    True label cut is:  0.0
    Arbitrary label cut is:  397.0
    

# The Volume Term

$$\mathbf{vol}(C_i)$$ as mentioned above measures how big some cluster $$C_i$$ is. Similar to how we calculated the cut term, we will collect all the values into arrays that are associated with each cluster, but instead we will sum the values within each array respectively:



```python
def vols(A, y):
        
        C_0_values = np.where(y == 0)[0] # Values in cluster 0
        C_1_values = np.where(y == 1)[0] # values in cluster 1
        
        return np.sum(A[C_0_values]), np.sum(A[C_1_values]) #Returns row sums within each cluster
```

Now that we have created our function to obtain the volume term, we are now able calculate $$N_{\mathbf{A}}(C_0, C_1)$$ that we defined above, or the norm cut:


```python
def normcut(A, y):
    
    cut_term = cut(A, y) # calculate our cut term

    v_0, v_1 = vols(A, y) # calculate our volumes for each cluster
    
    return cut_term*(1/v_0+ 1/v_1) # Return our norm cut objective
```

Same as with the cut term, we will calculate the true norm cut as well as the norm cut associated to some random label `y_random` that we created above:


```python
np.random.seed(1234) # Set the seed for replication purposes

print("True norm cut is: ", normcut(A, y))
print("Arbitrary norm cut is: ", normcut(A, y_random))
```

    True norm cut is:  0.0
    Arbitrary norm cut is:  1.0462545950710016
    

And thus we have finished the first part to our journey in creating a spectral clustering algorithm! In the next section, we will work through a more efficient method of doing so that works will large data sets.

# 2. Optimizing our Norm Cut Function

Now that we have our norm cut as explained above, we are going to have to optimize it a bit! 

In our current state, we take small values when the clusters are joined by relatively small amount of entries in our similarity matrix, as well as the clusters not being too small.

However, our labelling vector `y` is incredibly difficult to calculate efficiently, so we'll have to resort to some math trickery.

$$
z_i = 
\begin{cases}
    \frac{1}{\mathbf{vol}(C_0)} &\quad \text{if } y_i = 0 \\ 
    -\frac{1}{\mathbf{vol}(C_1)} &\quad \text{if } y_i = 1 \\ 
\end{cases}
$$

So, to explain the above, if we are in cluster $$C_{0}$$, we return a positive value of the volume term associated to it, and for cluster $$C_{1}$$, we return the negative value of the volume term associated to it for ease of deciphering which cluster we are currently in.

To show that what we are doing so far is correct, we will use the below  and check if it is indeed the same as our norm cut function above:

$$\mathbf{N}_{\mathbf{A}}(C_0, C_1) = 2\frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}}\;, (1)$$

where $$\mathbf{D}$$ is the diagonal matrix with nonzero entries $$d_{ii} = d_i$$, and  where $$d_i = \sum_{j = 1}^n a_i$$ is the degree (row-sum) from before.  

We will create a function `transform(A, y)` to compute the piecewise-define `z` described above given our similarity matrix $$\mathbf{A}$$ and cluster vector $$\mathbf{y}$$:


```python
def transform(A, y):
    z = np.zeros(shape = y.size) # Create a vector with the same size as our cluster vector
    
    v_0, v_1 = vols(A, y) # Define our volume terms for each cluster
    
    # Define z as described in the piecewise function depending on what cluster it is in
    z[y == 0] = 1/v_0 
    z[y == 1] = -1/v_1
    
    return z
```

Now we check that our calculated vector in works with equation `(1)` is indeed valid:


```python
D = np.diag(A.sum(axis = 0)) # Creates diagonal matrix with non-zero entries d_ii = d_i where d_i is the degree

z = transform(A, y) # z as noted above

norm_cut_numer = 2 * (z @ (D - A) @ z) # numerator of our norm cut equation
norm_cut_denom = z @ D @ z # denominator of our norm cut equation
norm_cut_eq = norm_cut_numer/norm_cut_denom

# Now we check if our normcut function relates to the equation defined above by using np.isclose():
np.isclose(normcut(A, y), norm_cut_eq)
```




    True



Lastly, we will check the identity $$\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$, where $$\mathbb{1}$$ is the vector of `n` ones (i.e. `np.ones(n)`). This identity effectively says that $$\mathbf{z}$$ should contain roughly as many positive as negative entries. 


```python
e = np.ones(n) 

np.isclose(z @ D @ e, 0)
```




    True



# 3. Further optimizations!

In the last part, we saw that the problem of minimizing the normcut objective is mathematically related to the problem of minimizing the function 

$$ R_\mathbf{A}(\mathbf{z})\equiv \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}} $$

subject to the condition $$\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$. 

By substituting $$\mathbf{z}$$ for its orthogonal complement relative to $$\mathbf{D}\mathbf{1}$$, we can actually make this condition imbedded into our optimization. In the code below, I define an `orth_obj` function which handles this so you don't have to do it :) 

We will further use the `minimize` function from `scipy.optimize` to minimize the function `orth_obj` with respect to $$\mathbf{z}$$. 

```python
def orth(u, v):
    return (u @ v) / (v @ v) * v

e = np.ones(n) 

d = D @ e

def orth_obj(z):
    z_o = z - orth(z, d)
    return (z_o @ (D - A) @ z_o)/(z_o @ D @ z_o)
```


```python
from scipy.optimize import minimize

z_ = minimize(orth_obj, z)["x"]
```

As we defined our vector `z_`, only the sign of $$i^{th}$$ entry actually contains information about the cluster label; a positive $$i^{th}$$ value is in $$C_{0}$$, while a negative value shows the entry is in cluster $$C_{1}$$. 

We will plot our original crescent data with the progressions we've made thus far, where we use one color for points such that `z_[i] < 0` and another color for points such that `z_[i] >= 0`: 


```python
plt.scatter(X[:,0], X[:,1], c = np.where(z_ < 0, 0, 1)) 
# np.where(z_ < 0, 0, 1) allows us to define the colors between our clusters
```




    <matplotlib.collections.PathCollection at 0x1f9c2df5190>




    
![blog_post_2_plot5.png](/images/blog_post_2_plot5.png)
    


And we can say safely that it does look like we came close to closely clustering the data!

## 4. Different Optimizations!

As you can see from the above, we calculated our own orthogonal `z` above, however it is a bit too inefficient to be used in all situations. So rather, we will work with our clustering algorithm utilizing eignvalues and eigenvectors of our matrices.

Recall that we had to minimize the equation (1) we had in section 2 with respect to our $$\mathbf{z}$$, and subject to the condition $$\mathbf{z^{T}D\mathbb{1}}$$.

By the Rayleigh Rit Theorem, the act of minimizing our vector $$\mathbf{z}$$ must be the solution to the smallest eigenvalue $$\lambda$$ of the generalized eigenvalue problem:

$$ \mathbf{D}^{-1}(\mathbf{D} - \mathbf{A}) \mathbf{z} = \lambda \mathbf{z}\;, \quad \mathbf{z}^T\mathbb{1} = 0\;.$$

However, note that $$\mathbb{1}$$ is arbitrarily the smallest eigenvector, so we want the our $$\mathbf{z}$$ with the second-smallest eigenvalue instead!

In the below, we will construct a matrix $$\mathbf{L} = \mathbf{D}^{-1}(\mathbf{D} - \mathbf{A})$$, which is often called the (normalized) *Laplacian* matrix of the similarity matrix $$\mathbf{A}$$. Find the eigenvector corresponding to its second-smallest eigenvalue, and call it `z_eig`. Then, plot the data again, using the sign of `z_eig` as the color


```python
L = np.linalg.inv(D) @ (D - A)

ev_sort = np.linalg.eig(L)[0].argsort() # gives us the indices of the sorted eigenvalues
z_eig = (np.linalg.eig(L)[1])[:, ev_sort][:,1] # the eigenvector associated to the second smallest eigenvalue
```

And now we plot the same data above using the signs of our eigenvalues `z_eig` as the colors

{::options parse_block_html="true" /}
<div class="got-help">
For calculating our second smallest eigenvector, I originally had:

```python
L = np.linalg.pinv(D) @ (D - A)
ev, U = np.linalg.eig(L)
ev_sort = ev.argsort()  
z_eig = U[:,ev_sort][:,1]
```

and fixed it up a bit here: 

```python
L = np.linalg.inv(D) @ (D - A)

ev_sort = np.linalg.eig(L)[0].argsort() # gives us the indices of the sorted eigenvalues
z_eig = (np.linalg.eig(L)[1])[:, ev_sort][:,1] # the eigenvector associated to the second smallest eigenvalue
```

making the block of code a lot more concise, removing unecessary declarations that was recommended by a peer-review :)
</div>
{::options parse_block_html="false" /}

```python
plt.scatter(X[:,0], X[:,1], c = np.where(z_eig < 0, 0, 1))
```




    <matplotlib.collections.PathCollection at 0x1f9c2e4c2e0>




    
![blog_post_2_plot6.png](/images/blog_post_2_plot6.png)
    


# 5. An All-In-One Function for Our Spectral Clustering Algorithm

Using everything we went through and learned in the past few sections, we will create a function combining everything to achieve the clustering effect we've been working for.


```python
def spectral_clustering(X, epsilon):
    """
    We create two matrices: an empty similarity matrix A with shape n x n, as well as the matrix noting (X[i] - X[j])**2
    We then modify our similarity matrix to be 1 if it's within our epsilon and 0 if not/along the diagonal
    
    D is the diagonal matrix with nonzero entries with d_ii = d_i, the row-sum of our matrix A
    L is our normalized Laplacian matrix as described previously, using the pseudoinverse of D
    
    ev, U are our eigenvalues, as well as our eigenvectors respectively
    ev_sort is our eigenvalues sorted
    z_eig returns the eigenvector associated to the second-smallest eigenvector
    
    we return labels based on the eigenvector above with those with negative values in C_0, positive values in C_1
    """
    A = np.zeros(shape = (X.shape[0], X.shape[0])) # Creates an empty n x n matrix for us to populate
    
    P = pairwise_distances(X, metric = 'euclidean') # Our distance matrix from data set X

    A[P < epsilon**2] = 1 # If our distance matrix value is within our chosen epsilon, make entry 1 as we know everything else is 0 otherwise by the construction of our empty A

    np.fill_diagonal(A, 0) # Makes sure our diagonal entries are 0
    
    D = np.diag(A.sum(axis = 0)) # Construct our matrix D
    L = np.linalg.inv(D) @ (D - A) # Construct our Laplacian matrix

    ev_sort = np.linalg.eig(L)[0].argsort() # gives us the indices of the sorted eigenvalues
    e_vectors = np.linalg.eig(L)[1] # matrix of our eigenvectors
    z_eig = e_vectors[:, ev_sort][:,1] # the eigenvector associated to the second smallest eigenvalue
    
    return np.where(z_eig < 0, 0, 1)
```

# 6. Using our All-In-One Function

We are going to be using `sklearn`'s datasets to make crescent shapes for us to divide our clusters! Now that we have an efficient algorithm developed, we can use larger data sets, i.e. `n = 1000`, and so on. 

Now that we don't have to worry about the size of our data or the shape of it to a large extent, you could experiment on your own with varied noise values and epsilon. When I worked through it myself, I saw that the noise is similar to a noise in an image; very grainy and hard to distinguish between sections in a way.


```python
np.random.seed(7726) #Set seed for reproducibility

n2 = 1000 # The number of data points we wish to create 
X2, y2 = datasets.make_moons(n_samples=n2, shuffle=True, noise=0.1, random_state=None)
# We can see through experimentation noise creates more chaotic data; essentially less visibility between clusters in my eyes
z2 = spectral_clustering(X2, epsilon = 0.45) # Creates our label vector
plt.scatter(X2[:,0], X2[:,1], c = z2) # Plots it
```




    <matplotlib.collections.PathCollection at 0x1f9c2ea5a00>




    
![blog_post_2_plot7.png](/images/blog_post_2_plot7.png)
    


To work through a new set of data, we will try to separate the clusters with a bullseye-shaped plot as shown below:


```python
np.random.seed(1234)

n = 1000
X, y = datasets.make_circles(n_samples=n, shuffle=True, noise=0.05, random_state=None, factor = 0.4)
plt.scatter(X[:,0], X[:,1])
```




    <matplotlib.collections.PathCollection at 0x1f9c2ef8970>




    
![blog_post_2_plot8.png](/images/blog_post_2_plot8.png)
    


Recall the previous method we learned of before we worked through how to develop or algorithm, K-means from `sklearn`. We can see that for the bullseye it doesn't do a very good job of distinguishing between the inner and outer circles:


```python
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```




    <matplotlib.collections.PathCollection at 0x1f9c2f4d3a0>




    
![blog_post_2_plot9.png](/images/blog_post_2_plot9.png)
    


As our `spectral_clustering(X, epsilon)` function takes in the data and an epsilon we will have to work through various epsilon values to see what works for this data set in particular. So, we will first test the extremes of: `epsilon = 0.1` and `epsilon = 1`


```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.1)) 
```




    <matplotlib.collections.PathCollection at 0x1f9c2f97d30>




    
![blog_post_2_plot10.png](/images/blog_post_2_plot10.png)
    



```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 1)) 
```




    <matplotlib.collections.PathCollection at 0x1f9c2fed8e0>




    
![blog_post_2_plot11.png](/images/blog_post_2_plot11.png)
    


We see that for `epsilon = 0.1`, it essentially does no clustering at all, it looks arbitrary in how it choses between each cluster, whereas with `epsilon = 1`, it almost splits the data right down the middle. 

So, next we will try something in the middle-ground, `epsilon = 0.5`


```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.5)) 
```




    <matplotlib.collections.PathCollection at 0x1f9c3042640>




    
![blog_post_2_plot12.png](/images/blog_post_2_plot12.png)
    


Similar to `epsilon = 0.1`, a value of `epsilon = 0.5` seems to do absolutely nothing.

So instead, we will go in increments of $$\pm$$ 0.1 to see what effect that does instead:


```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.4)) 
```




    <matplotlib.collections.PathCollection at 0x1f9c309a160>




![blog_post_2_plot13.png](/images/blog_post_2_plot13.png)
    



```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.6)) 
```




    <matplotlib.collections.PathCollection at 0x1f9c30e1e80>




    
![blog_post_2_plot14.png](/images/blog_post_2_plot14.png)
    


Interestingly enough, we can see that those values work out in separating these the two ellipses! 

Just to understand a bit more of what is going on, we could go to `epsilon = 0.3` and `epsilon = 0.7` to maybe see if we can see a pattern in changing the epsilon values for our bullsye. It's a bit tedious to keep verifying epsilon values, so I'll just explain my own findings!

When I went down to `epsilon = 0.3`, it had the same effect as `epsilon = 0.1`; no clear spearation between the two ellipses, whereas `epsilon = 0.7` worked as well as `epsilon = 0.4/0.6`. Once we went to `epsilon = 0.8` it had the same effect as choosing `epsilon = 1`

That concludes the end of my blog post about the spectral clustering algorithm! I hope it was helpful and informative.
