---
layout: post
title: Blog Post 3
permalink: /projects/instructional_blogs/blog-post-three/
---

Today, we are going to work to answer the question:

> When detecting fake news, is it most effective to focus on only the title of the article, the full text of the article, or both?

We will answer and work towards answering this question using machine-learning with `Tensorflow`, an open source platform for machine learning created by Google.

We'll briefly explain the process of modelling for machine learning, then we'll work to explain what `Tensors` are, and how our neural networks work.

{::options parse_block_html="true" /}
<div class="gave-help">
Some of the blog posts that I assessed were very good in content/explanations/programming knowledge, but didn't really give much background and context to machine learning and Tensorflow. I gave feedback to maybe add a little information about what these things are and how they work especially as this blog post was aimed for us students three weeks before we began this assignment. 
</div>
{::options parse_block_html="false" /}

Firstly, there are essentially four components to modelling with Machine Learning. There are the:
* Predictors : The information that our model wants to predict
* Target Variable : The part of the data that the model aims to predict using our predictors.
* Model : A function:

$$f : \textrm{Predictors} \longrightarrow \textrm{Target Variable}$$

that maps our Predictors to the predictions of the Target Variable.
* Loss Function : A measure of how much error our model creates when predicting our Target Variables using the Predictors. We want to minimize this. This is used to compare various models.

Now to explain tensors. At its most basic level, `Tensors` are just arrays! It encodes some multi-dimensional data which allows us to answer questions and problems involving huge numbers of data easily.

In terms of a neural network, it's essentially a loose model of the human brain that takes a set of `layers` to recognize patterns in our data. 

A `Layer` is a container that recieves some input, and transforms it using a function of some sort which then passes this new `Tensor` to the next layer to then be layered again and again.

This is usually done in a myriad of `layers` to get a more sophisticated and smart classification system of our data to better understand it. This is done mathematically with the inputted data and some `weight`-values of the layer by taking the inner product between the two. The `weight`-values are determined by some chosen activation function that is being used and the number of inputs to the node. 

These layers are built on top of one each other by composition to produce a model to fit our data.

Going back to the data, in the below, we will be using data with columns:
* `title`: title of the article
* `text` : text of the article
* `fake` : 0 if the article is not fake, 1 if it is


So in the below, we are going to import the following packages and import our data:

```python
import pandas as pd #
import tensorflow as tf
import numpy as np

# Both packages below are used in our standardization of our data (which will be explained later on)
import re
import string 

from tensorflow.keras.layers.experimental.preprocessing import TextVectorization # Used to vectorize our text from string --> int
from tensorflow.keras.layers.experimental.preprocessing import StringLookup

from tensorflow import keras # To create, use, and inspect our model
from tensorflow.keras import layers # To create the layers to our neural network
```

```python
train_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true" # Data file

fn_df = pd.read_csv(train_url) # Read in our data

fn_df = fn_df.drop(columns = ["Unnamed: 0"]) # Drop unnecesary column

fn_df
```

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>title</th>
      <th>text</th>
      <th>fake</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Merkel: Strong result for Austria's FPO 'big c...</td>
      <td>German Chancellor Angela Merkel said on Monday...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Trump says Pence will lead voter fraud panel</td>
      <td>WEST PALM BEACH, Fla.President Donald Trump sa...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>JUST IN: SUSPECTED LEAKER and “Close Confidant...</td>
      <td>On December 5, 2017, Circa s Sara Carter warne...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Thyssenkrupp has offered help to Argentina ove...</td>
      <td>Germany s Thyssenkrupp, has offered assistance...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Trump say appeals court decision on travel ban...</td>
      <td>President Donald Trump on Thursday called the ...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>22444</th>
      <td>ALARMING: NSA Refuses to Release Clinton-Lynch...</td>
      <td>If Clinton and Lynch just talked about grandki...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>22445</th>
      <td>Can Pence's vow not to sling mud survive a Tru...</td>
      <td>() - In 1990, during a close and bitter congre...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22446</th>
      <td>Watch Trump Campaign Try To Spin Their Way Ou...</td>
      <td>A new ad by the Hillary Clinton SuperPac Prior...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>22447</th>
      <td>Trump celebrates first 100 days as president, ...</td>
      <td>HARRISBURG, Pa.U.S. President Donald Trump hit...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22448</th>
      <td>TRUMP SUPPORTERS REACT TO DEBATE: “Clinton New...</td>
      <td>MELBOURNE, FL is a town with a population of 7...</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
<p>22449 rows × 3 columns</p>
</div>

So now that we have our data imported, we are going to want to clean up our data, namely the titles and text of the articles in our dataframe, as well as return a workable dataset.

So, in the function `make_dataset(data)` below, we are going to:
* Remove stop words from the dataframes `text` and `title` columns
  * `stopwords` are a set of commonly used words in any language that will not be helpful for our machine learning algorithm to analyze
* Construct and return a `tf.data.Dataset` with two inputs (`(title, text)`), and one output (`fake`), all coming from our dataframe.

You can see when we call `tf.data.Dataset.from_tensorslices` we input two dictionaries, where:
1. The first dictionary specifies components in the predictor data: the `title` and `text` column
2. The second dictionary specifies the component of our target data: the `fake` column 
These correspond to the requirement above of returning our Tensorflow `Dataset` with two inputs and one output from our dataframe.

This function will be applied to our training dataframe to create a `Dataset`, whereas we will `batch` our `Dataset`, which will make our model train on smaller chunks of data. Although it could sometimes reduce the accuracy of our model, it will make things more convenient by greatly increasing the speed of our training. 

So without further ado:

{::options parse_block_html="true" /}
<div class="got-help">
Created a docstring for our `make_dataset(df)` function. I create docstrings for standardization, create_vectorize_layer, and create_string_input as well. All will be noted by the same box before the function is shown.
</div>
{::options parse_block_html="false" /}

```python
# Import stopwords with nltk.
from nltk.corpus import stopwords 
import nltk
nltk.download('stopwords') # Download our stopwords list in case not available
def make_dataset(df):
  """
  PURPOSE
  -----
  This function works to:
  1. remove stopwords
  2. convert out dataset into tf.data.Dataset
  3. batch our data for quicker training
  
  INPUT
  -----
  df : Our dataset containing the title, text, and fake data necessary for our goal

  OUTPUT
  -----
  It returns a batched tf.data.Dataset with two inputs (text,title) and one output (fake)
  """

  stop_words = stopwords.words('english') # Assign our stop_words array

  df['title'] = df['title'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))
  # Remove our stopwords in the titles
  df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))
  # Remove our stopwords in the text

  curr_data = tf.data.Dataset.from_tensor_slices( # Create our tf.data.Dataset with:
      (
        { # Two inputs, (title, text)
            "title" : df[["title"]],
            "text" : df[["text"]]
        },
        { # One output, (fake)
            "fake" : df[["fake"]]
        }
      )
  )
  
  curr_data.batch(100) # Batch to train faster

  return curr_data # Return our newly made tf.data.Dataset
```

To where we can finally convert and make our dataset into type `tf.data.Dataset`!

```python
data = make_dataset(fn_df)

data
```
    <TensorSliceDataset shapes: ({title: (1,), text: (1,)}, {fake: (1,)}), types: ({title: tf.string, text: tf.string}, {fake: tf.int64})>

In the below, to get a general idea of how our Tensorflow `Dataset` is formatted and set up, we'll display our inputs and output as set up in the above code:

```python
for title_text, fake in data.take(3): # Tuple of our data of inputs and outputs
  print(title_text)
  print(fake)
  print("")
```

    {'title': <tf.Tensor: shape=(1,), dtype=string, numpy=
    array([b"Merkel: Strong result Austria's FPO 'big challenge' parties"],
          dtype=object)>, 'text': <tf.Tensor: shape=(1,), dtype=string, numpy=
    array([b'German Chancellor Angela Merkel said Monday strong showing Austria anti-immigrant Freedom Party (FPO) Sunday election big challenge parties. Speaking news conference Berlin, Merkel added hoping close cooperation Austria conservative election winner Sebastian Kurz European level.'],
          dtype=object)>}
    {'fake': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>}
    
    {'title': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Trump says Pence lead voter fraud panel'], dtype=object)>, 'text': <tf.Tensor: shape=(1,), dtype=string, numpy=
    array([b'WEST PALM BEACH, Fla.President Donald Trump said remarks broadcast Sunday would put Vice President Mike Pence charge commission probe believes voter fraud last November\'s election. There overwhelming consensus among state officials, election experts, politicians voter fraud rare United States, Trump repeatedly said thinks perhaps millions votes cast Nov. 8 election fraudulent. "I\'m going set commission headed Vice President Pence we\'re going look very, carefully," Trump told Fox News Channel\'s Bill O\'Reilly interview taped Friday. Trump, spending weekend Mar-a-Lago resort Palm Beach, Florida, captured presidency winning enough state-by-state Electoral College votes defeat Democrat Hillary Clinton. Still, Clinton popular vote nearly 3 million votes, piling overwhelming majority deeply Democratic states like California. This irked Trump result claimed voter fraud without evidence. Senate Majority Leader Mitch McConnell, Kentucky Republican, said \'s "State Union" election fraud occur "there evidence occurred significant number would changed presidential election." "And I think ought spend federal money investigating that. I think states take look issue," said.'],
          dtype=object)>}
    {'fake': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>}
    
    {'title': <tf.Tensor: shape=(1,), dtype=string, numpy=
    array([b'JUST IN: SUSPECTED LEAKER \xe2\x80\x9cClose Confidant\xe2\x80\x9d James Comey Has Been Reassigned From His Post As TOP FBI Lawyer [VIDEO]'],
          dtype=object)>, 'text': <tf.Tensor: shape=(1,), dtype=string, numpy=
    array([b'On December 5, 2017, Circa Sara Carter warned would major shake-up FBI Inspector General report completed. So far, Sara Carter right everything reported on, relates Mueller investigation. In below, Carter tells Sean Hannity believes FBI major shake-up soon 27 leakers IG looking at! Yes, 27 leakers!Sara Carter: We going see parts report December (end month). We going see parts report coming January. And looking Peter Strzok. They looking Comey. They looking 27 leakers. It would surprise shake-up FBI housecleaning.Watch:Is FBI former top attorney, James Baker, one first leaker casualties? James Baker, FBI leading lawyer confidante fired FBI Director James Comey, reassigned post, agency top personnel high scrutiny.Baker told colleagues assume different duties bureau, Washington Post reported.Baker oversees bureau Office General Counsel received awards George H.W. Bush Award Excellence counter-terrorism 2006.He also subject leak investigation summer Attorney General Jeff Sessions ordered crackdown leakers.The FBI comment asked Baker reassigned would doing.His reassignment comes time increased scrutiny pressure agency, following release private text messages agents working Hillary Clinton email probe. Daily Mail Three sources, knowledge investigation, told Circa Baker top suspect ongoing leak investigation, Circa able confirm details national security information material allegedly leaked.A federal law enforcement official knowledge ongoing internal investigations bureau told Circa, bureau scouring leakers lot investigations. The revelation comes Trump administration ramped efforts contain leaks within White House within national security apparatus.Baker close confidant former FBI Director James Comey, recent media reports suggested reportedly advising then-FBI director legal matters following private meetings former director February President Trump Oval Office.Baker appointed FBI general counsel Comey 2014 long distinguished history within intelligence community.'],
          dtype=object)>}
    {'fake': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>}
    

In the below, we will take our data and shuffle it in order to split our train/val/test datasets.

To explain,
* `train`: The set of data we use to train our model by pairing the input with the expected output
  * In this case, our input is the `title` and `text`, and the expected output is the `fake` label
*`val` and `test`:  The set of data we use to estimate how well our model has been trained and to estimate model properties

We will take 70% of the data to train, 20% to validate, and the rest of the data to test.

Similar to the above, we batch train, validation, and test data to train our data faster as well:


```python
data = data.shuffle(buffer_size = len(data))

train_size = int(0.6*len(data))
val_size   = int(0.2*len(data))

train = data.take(train_size).batch(20)
val   = data.skip(train_size).take(val_size).batch(20)
test  = data.skip(train_size + val_size).batch(20)
```

In the above, we can see how the data and how it is stored for some 3 data points in our `Dataset`.

* The inputs contain our `title` and `text` information as well as their data type, `dtype`, which is of course `string`.
* The output contains our `fake` labeller, which has `dtype=int64` as a `0` indicates valid information whereas a `1` denotes fake news.

In the below, we will work to `standardize` and `vectorize` our inputs. What this means is that:
* `standardize` : It takes our text and removes any unnecessary characters that aren't useful for our modeling. So in our case, we work to:
  * convert all text to lowercase
  * remove punctuation
* `vectorization` : Generally, it refers to representing text as a vector. In our case, we will rank words by frequency and replace said word with their frequency rank by using the `TextVectorization` function, which is a part of `keras`.
  * In our `TextVectorization` usage, we use:
    * `standardize = standardization` to standardize our text using our created function
    * `max_tokens = 2000` to specify our max dictionary count
    * ``output_mode = `int` `` to specify that we want our vectorized data to output as integers
    * `output_sequence_length = 500` to specify the max length of our outputs

We then have to map our training data to this new vectorized data, as well as `adapt` our data, which will analyze the dataset, determine the frequency of individual string values, and create a 'vocabulary' from them.

This is exactly what we do below dynamically with our `create_vectorize_layer` function:

{::options parse_block_html="true" /}
<div class="got-help">
Created a docstring for our `standardization(input_data)` and `create_vectorize_layer(train, feature)` functions. I create docstrings for make_dataset and create_string_input as well. All will be noted by the same box before the function is shown.
</div>
{::options parse_block_html="false" /}

{::options parse_block_html="true" /}
<div class="gave-help">
From some of the student's blog posts that I assessed, they hadn't created general functions that they could use for creating their vectorized layers or some of their creating of their keras.Inputs, so I gave them feedback to maybe create functions to minimize code duplication.

From feedback that I got I was advised to do the same for creating my models/layers (a bit hypocritical, I know), but I felt it wouldn't have flowed as well with the blog post and the whole structure of what I had written, etc.
</div>
{::options parse_block_html="false" /}

```python
def standardization(input_data): 
    """
    PURPOSE
    -----
    This function works to:
    1. make all text in given input_data lowercase
    2. removes all punctuation
  
    INPUT
    -----
    input_data : The data we wish to standardize; in our case it will be our training data containing title + text data

    OUTPUT
    -----
    It returns our input data with all text lowercase + no punctuation
    """

    lowercase = tf.strings.lower(input_data) # Make text lowercase 
    no_punctuation = tf.strings.regex_replace(lowercase, 
                                  '[%s]' % re.escape(string.punctuation),'')
    # Remove punctuation
    return no_punctuation


def create_vectorize_layer(train, feature):
    """
    PURPOSE
    -----
    This function works to create a TextVectorization object which provides us a 
    frequency ranking for words contained in our data up to a max_tokens amount of vocabulary words
  
    INPUT
    -----
    train   : The training data
    feature : Name of the input in our tf.data.Dataset we wish to vectorize

    OUTPUT
    -----
    Our vectorized layer
    """
   # Allow us to vectorize a particular input (feature) in our Dataset 
  vectorize_layer = TextVectorization(
      standardize = standardization,
      max_tokens = 2000, # Take the top 2000 frequent words
      output_mode = 'int', # Convert to int
      output_sequence_length = 500
  )
  vectorize_layer.adapt(train.map(lambda x, y : x[feature]))
  # Create our vectorized layer for the particualr input
  return vectorize_layer

vectorize_title = create_vectorize_layer(train, 'title')
vectorize_text = create_vectorize_layer(train, 'text')
```

As we have multiple inputs, we are going to want to specify each `keras.Input` for our model as we have two distinct sets of predictor data: `title` and `text`.

We can see the parameters of `keras.Input` where:
* `shape` is the shape of the data
* `name` is the name of the input
* `dtype` is the data type of the input tensors

So below we have a function to create inputs pending the input label:

{::options parse_block_html="true" /}
<div class="got-help">
Created a docstring for our `create_string_input(name)` function. I create docstrings for make_dataset, standardization, and create_vectorize_layer as well. All will be noted by the same box before the function is shown.
</div>
{::options parse_block_html="false" /}

```python
def create_string_input(name):
"""
    PURPOSE
    -----
    This function works to create a keras.Input to be used for our modelling/training
  
    INPUT
    -----
    name : name of our input

    OUTPUT
    -----
    Our keras.Input object
    """
  return keras.Input(
      shape = (1,),
      name = name,
      dtype = "string"
  )

title_input = create_string_input('title')
text_input = create_string_input('text')

title_input, text_input
```
    (<KerasTensor: shape=(None, 1) dtype=string (created by layer 'title')>,
     <KerasTensor: shape=(None, 1) dtype=string (created by layer 'text')>)

Now that we have all of our data ready and processed, we are finally ready to answer the proposed question!

We will create three different models ...
1. Using only our `title` input
2. Using only our `text` input
3. Using both `title` and `text` inputs together.

We will create a pipeline by stacking layers on top of each other as described in the beginning of the blog post.

# Title Input

So now, we will create our pipeline for our article `title`. In the below we use various hidden layers. For our `title` input specifically, we use:
* `Embedding` : Mostly used for `word embedding`. To explain, `word embedding` is a way to represent words using a dense vector representation. The position of some word is based on the words that surround it when it is used. If used, it is always defined as the first hidden layer of a neural network.
  * In our usage of `Embedding`, we utilize two inputs:
    * `input_dim` : The size of the vocabulary in our text data. Recall in our creation of our `TextVectorization` our `max_tokens` (our ranked vocabulary size) is defined as `2000`, so that's what we will use.
    * `output_dim` : The size of the output vectors of this specific layer for each word. The choice of this number varies between testing and our data, so work with various values to see how it changes your data. In our case, we use `40`. I saw that I had more success with greater output values than smaller ones (choosing from range [20, 45])
* `GlobalPooling1D` : Global Average Pooling is an operation that works to generate a feature map for each category of our classification task, namely whether an article is fake news or not in our case.
* `Dense` : Most commonly used in models, as the name itself suggests, it takes the neurons of our neural network that connects all the neurons from those of the previous layer.
  * We use this layer on two instances.
    * 1. `layers.Dense(32, activation = 'relu')`
      * We have `units` specified as `32`, which specifies the dimensionality of the output space. It is almost like controlling the amount of hidden features learned by this specific layer. I had around the same success with `32` and `64` with choosing `32` being a bit faster to fit our model. With `` units = `16` ``, I had slightly worse success.
      * We choose our activation function as `relu`, which is responsbile for transforming our summed weighted input into the activation of the nodes/output for the specified input
        * `relu` in particular is commonly used as it makes the data easier to train and achieves better performance usually. In particular it is a piecewise linear function that outputs the input if it's positive, whereas otherwise it will output 0.
    * 2. `layers.Dense(1, activation = 'sigmoid', name = 'fake')`
      * we've chosen the final output layer to output 1 number as there is a single classification; whether the news article is fake or not
      * We choose an activation of `sigmoid` as a sigmoid function varies between 0 and 1, exactly what our `fake` data entails.
      * We choose ``name = `fake` `` as that is the name of our output layer.


```python
title_features = vectorize_title(title_input)
title_features = layers.Embedding(2000, 35)(title_features)
title_features = layers.GlobalAveragePooling1D()(title_features)
title_features = layers.Dense(32, activation = 'relu')(title_features)

title_output = layers.Dense(1, activation = 'sigmoid', name = 'fake')(title_features)
```

To which we will create our model using `keras` with `inputs` of our vectorized `title` data, and output of the single `Dense` layer as described previously:


```python
title_model = keras.Model(
    inputs = [title_input],
    outputs = title_output
)
```

Now we must compile our model for our `title`s. 

To explain the parameters chosen:
* Recalling Loss Functions above, we choose `binary_crossentropy` as our `loss` function. `SparseCategoricalCrossentropy` is usually used for basic classification tasks, which produces a category index of the most liekly matching category. As we have a binary classification, I found that using `binary_crossentropy` gave me the best results. 
* We use optimizer `adam`, which is an optimization algorithm for training deep learning models. 
* We choose `metrics` to be `` `accuracy` `` so we can see how accurate our model is of course

So here it is:


```python
title_model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])
```

So now that we have our model compiled, we can now use it on our training data!

We input... :
* `train`ing data
* `val`idation data
* `epochs` is the number of times we run our model 
* `Verbose` displays the `loss` and `accuracy` of our model per each `Epoch`


```python
title_history = title_model.fit(train, 
                    validation_data=val,
                    epochs = 10, 
                    verbose = True)
```
    Epoch 1/10
    674/674 [==============================] - 3s 3ms/step - loss: 0.6825 - accuracy: 0.5670 - val_loss: 0.4347 - val_accuracy: 0.9412
    Epoch 2/10
    674/674 [==============================] - 2s 2ms/step - loss: 0.3103 - accuracy: 0.9391 - val_loss: 0.1369 - val_accuracy: 0.9657
    Epoch 3/10
    674/674 [==============================] - 2s 2ms/step - loss: 0.1232 - accuracy: 0.9658 - val_loss: 0.0862 - val_accuracy: 0.9735
    Epoch 4/10
    674/674 [==============================] - 2s 2ms/step - loss: 0.0868 - accuracy: 0.9718 - val_loss: 0.0766 - val_accuracy: 0.9797
    Epoch 5/10
    674/674 [==============================] - 2s 2ms/step - loss: 0.0696 - accuracy: 0.9786 - val_loss: 0.0817 - val_accuracy: 0.9746
    Epoch 6/10
    674/674 [==============================] - 2s 2ms/step - loss: 0.0587 - accuracy: 0.9812 - val_loss: 0.0555 - val_accuracy: 0.9849
    Epoch 7/10
    674/674 [==============================] - 2s 2ms/step - loss: 0.0617 - accuracy: 0.9792 - val_loss: 0.0583 - val_accuracy: 0.9828
    Epoch 8/10
    674/674 [==============================] - 2s 2ms/step - loss: 0.0508 - accuracy: 0.9811 - val_loss: 0.0512 - val_accuracy: 0.9851
    Epoch 9/10
    674/674 [==============================] - 2s 2ms/step - loss: 0.0542 - accuracy: 0.9826 - val_loss: 0.0400 - val_accuracy: 0.9871
    Epoch 10/10
    674/674 [==============================] - 2s 2ms/step - loss: 0.0499 - accuracy: 0.9821 - val_loss: 0.0420 - val_accuracy: 0.9857
    
```python
from matplotlib import pyplot as plt
plt.plot(title_history.history["accuracy"], label = "training")
plt.plot(title_history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```

![title_output.png](/images/title_output.png)

{::options parse_block_html="true" /}
<div class="got-help">
Made changes from both assessments with regard to adding axes and a legend to distinguish between our validation and training data.

We go from:

```python
from matplotlib import pyplot as plt
plt.plot(title_history.history["accuracy"])
plt.plot(title_history.history["val_accuracy"])
```

to:

```python
from matplotlib import pyplot as plt
plt.plot(title_history.history["accuracy"], label = "training")
plt.plot(title_history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```
</div>
{::options parse_block_html="false" /}

```python
title_model.evaluate(test, verbose = 2)
```
    225/225 - 0s - loss: 0.0377 - accuracy: 0.9880
    [0.037673674523830414, 0.9879759550094604]

So you can see that our model worked very well with our training and validation data of a 98.21% accuracy and 98.57% accuracy respectively, and using our model with the test data, we were able to obtain 98.79% accuracy with a very minimal loss of 0.03767! 

Now it is time to create a model to be used with our `text` data only!

# Text Input

So now, we will create our pipeline for our article `text`. In the below we use the same various hidden layers as our `title` pipeline, with some adjustments to the units or output dimensions.

I found better results to have greater units and output dimension rather than less for our text data, perhaps because there are more variety of words in the text as opposed to the titles. I'm not 100% sure on this, but it seems logical.

The process of creating our layers, creating our model, compiling our model, and fitting our model is the exact same as for the `title` input.

```python
text_features = vectorize_text(text_input)
text_features = layers.Embedding(2000, 60)(text_features)
text_features = layers.GlobalAveragePooling1D()(text_features)
text_features = layers.Dense(64, activation = 'relu')(text_features)

text_output = layers.Dense(1, activation = 'sigmoid', name = 'fake')(text_features)
```

```python
text_model = keras.Model(
    inputs = [text_input],
    outputs = text_output
)
```

```python
text_model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])
```

```python
text_history = text_model.fit(train, 
                    validation_data=val,
                    epochs = 10, 
                    verbose = True)
```
    Epoch 1/10
    674/674 [==============================] - 4s 5ms/step - loss: 0.4950 - accuracy: 0.7529 - val_loss: 0.1275 - val_accuracy: 0.9637
    Epoch 2/10
    674/674 [==============================] - 3s 5ms/step - loss: 0.1138 - accuracy: 0.9679 - val_loss: 0.0816 - val_accuracy: 0.9786
    Epoch 3/10
    674/674 [==============================] - 3s 5ms/step - loss: 0.0746 - accuracy: 0.9806 - val_loss: 0.0597 - val_accuracy: 0.9849
    Epoch 4/10
    674/674 [==============================] - 3s 5ms/step - loss: 0.0668 - accuracy: 0.9823 - val_loss: 0.0463 - val_accuracy: 0.9871
    Epoch 5/10
    674/674 [==============================] - 3s 5ms/step - loss: 0.0559 - accuracy: 0.9857 - val_loss: 0.0458 - val_accuracy: 0.9904
    Epoch 6/10
    674/674 [==============================] - 3s 5ms/step - loss: 0.0370 - accuracy: 0.9898 - val_loss: 0.0405 - val_accuracy: 0.9915
    Epoch 7/10
    674/674 [==============================] - 3s 5ms/step - loss: 0.0326 - accuracy: 0.9931 - val_loss: 0.0212 - val_accuracy: 0.9953
    Epoch 8/10
    674/674 [==============================] - 3s 5ms/step - loss: 0.0293 - accuracy: 0.9934 - val_loss: 0.0227 - val_accuracy: 0.9940
    Epoch 9/10
    674/674 [==============================] - 3s 5ms/step - loss: 0.0294 - accuracy: 0.9916 - val_loss: 0.0182 - val_accuracy: 0.9933
    Epoch 10/10
    674/674 [==============================] - 3s 5ms/step - loss: 0.0230 - accuracy: 0.9937 - val_loss: 0.0150 - val_accuracy: 0.9978

```python
from matplotlib import pyplot as plt
plt.plot(text_history.history["accuracy"], label = "training")
plt.plot(text_history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```

![text_output.png](/images/text_output.png)

{::options parse_block_html="true" /}
<div class="got-help">
Made changes from both assessments with regard to adding axes and a legend to distinguish between our validation and training data.

We go from:

```python
from matplotlib import pyplot as plt
plt.plot(text_history.history["accuracy"])
plt.plot(text_history.history["val_accuracy"])
```

to:

```python
from matplotlib import pyplot as plt
plt.plot(text_history.history["accuracy"], label = "training")
plt.plot(text_history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```
</div>
{::options parse_block_html="false" /}

```python
text_model.evaluate(test, verbose = 2)
```
    225/225 - 1s - loss: 0.0192 - accuracy: 0.9969
    [0.01921486109495163, 0.9968826770782471]

So you can see that our model worked very well, and even better than our `title` pipeline, with our training and validation data of a 99.337% accuracy and 99.78% accuracy respectively, and using our model with the test data, we were able to obtain 99.68% accuracy with a very minimal loss of 0.01921!

Now it is time to create a pipeline utilizing both inputs.

# Title and Text Input

The process of creating our pipeline will be slightly different as we have two inputs rather than one.

We will have to create our layers just as for the above two pipelines, but we will have to `concatenate` the outputs of both pipelines.

We will then pass the consolidated set through two more dense layers with the last outputting one number for our `fake` output.

Furthermore, we create two shared layers between the two inputs, particularly for `Embedding` and `GlobalAveragePooling1D()` so we don't have to declare two different `Embedding` layers and `GlobalAveragePooling1D()` layers for each input as they serve the same function.

Apart from those things, everything else is carried out in the same manner.

```python
shared_embedding = layers.Embedding(2000, 45)
shared_global_pooling = layers.GlobalAveragePooling1D()

title_features = vectorize_title(title_input)
title_features = shared_embedding(title_features)
title_features = shared_global_pooling(title_features)
title_features = layers.Dense(32, activation = 'relu')(title_features)


text_features = vectorize_text(text_input)
text_features = shared_embedding(text_features)
text_features = shared_global_pooling(text_features)
text_features = layers.Dense(64, activation = 'relu')(text_features)

main = layers.concatenate([title_features, text_features], axis = 1)
main = layers.Dense(32, activation = 'relu')(main)

output = layers.Dense(1, activation = 'sigmoid', name = 'fake')(main)
```

```python
model = keras.Model(
    inputs = [title_input, text_input],
    outputs = output
)
```

```python
keras.utils.plot_model(model)
```

![text_title_diagram.png](/images/text_title_diagram.png)

In the above visualization, we can see how the inputs are passed through the layers, especially with the shared layers.

```python
model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])
```

```python
history = model.fit(train, 
                    validation_data=val,
                    epochs = 10, 
                    verbose = True)
```
    Epoch 1/10
    674/674 [==============================] - 5s 6ms/step - loss: 0.4169 - accuracy: 0.7873 - val_loss: 0.1133 - val_accuracy: 0.9693
    Epoch 2/10
    674/674 [==============================] - 4s 6ms/step - loss: 0.0820 - accuracy: 0.9773 - val_loss: 0.0511 - val_accuracy: 0.9893
    Epoch 3/10
    674/674 [==============================] - 4s 6ms/step - loss: 0.0676 - accuracy: 0.9787 - val_loss: 0.0457 - val_accuracy: 0.9891
    Epoch 4/10
    674/674 [==============================] - 4s 6ms/step - loss: 0.0526 - accuracy: 0.9832 - val_loss: 0.0336 - val_accuracy: 0.9924
    Epoch 5/10
    674/674 [==============================] - 4s 6ms/step - loss: 0.0442 - accuracy: 0.9873 - val_loss: 0.0338 - val_accuracy: 0.9920
    Epoch 6/10
    674/674 [==============================] - 4s 6ms/step - loss: 0.0359 - accuracy: 0.9893 - val_loss: 0.0585 - val_accuracy: 0.9786
    Epoch 7/10
    674/674 [==============================] - 4s 6ms/step - loss: 0.0334 - accuracy: 0.9914 - val_loss: 0.0246 - val_accuracy: 0.9955
    Epoch 8/10
    674/674 [==============================] - 4s 6ms/step - loss: 0.0281 - accuracy: 0.9924 - val_loss: 0.0267 - val_accuracy: 0.9911
    Epoch 9/10
    674/674 [==============================] - 4s 6ms/step - loss: 0.0297 - accuracy: 0.9916 - val_loss: 0.0163 - val_accuracy: 0.9958
    Epoch 10/10
    674/674 [==============================] - 4s 6ms/step - loss: 0.0174 - accuracy: 0.9946 - val_loss: 0.0138 - val_accuracy: 0.9964

```python
model.evaluate(test, verbose = 2)
```
    225/225 - 1s - loss: 0.0168 - accuracy: 0.9958
    [0.016767676919698715, 0.9957693219184875]

```python
from matplotlib import pyplot as plt
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```

{::options parse_block_html="true" /}
<div class="got-help">
Made changes from both assessments with regard to adding axes and a legend to distinguish between our validation and training data.

We go from:

```python
from matplotlib import pyplot as plt
plt.plot(history.history["accuracy"])
plt.plot(history.history["val_accuracy"])
```

to:

```python
from matplotlib import pyplot as plt
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```
</div>
{::options parse_block_html="false" /}

![text_title_output.png](/images/text_title_output.png)

So you can see that our model worked very well again. Better than our `title` pipeline, and slightly worse than our `text` pipeline. With our training and validation data we obtained 99.46% accuracy and 99.64% accuracy respectively, and using our model with the test data, we were able to obtain 99.57% accuracy with a very minimal loss of 0.01676! 

Now it is time to create a pipeline utilizing both inputs.

# Model Evaluation

We can see that by a slight margin, our `text` model provided the greatest accuracy, albeit by such a small percentage.

So, we will use that specific model to evaluate some unseen test data that we will read in and use our `make_dataset` function to convert it:


```python
test_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true"

test_pd = pd.read_csv(test_url) # Read in our data

test_data = make_dataset(test_pd)
```


```python
text_model.evaluate(test_data, verbose = 1)
```

       29/22449 [..............................] - ETA: 40s - loss: 0.1551 - accuracy: 0.9310        

    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/functional.py:595: UserWarning: Input dict contained keys ['title'] which did not match any model input. They will be ignored by the model.
      [n for n in tensors.keys() if n not in ref_input_names])
    

    22449/22449 [==============================] - 36s 2ms/step - loss: 0.0753 - accuracy: 0.9804
    [0.07530527561903, 0.980400025844574]

So, we can see that if we used our text model as a fake news detector, we would be right around 98% of the time! That's pretty good I'd say :)

# Embedding Visualization

{::options parse_block_html="true" /}
<div class="got-help">
I hadn't gotten explicit feedback from peers with respect to the changes below, but I felt I was lacking a bit in this section and thought rather than copy from Prof. Chodrow's notes, I could just include them if the reader wanted more information on how embedding works and how we created our visuals.

In particular I add this sentence:

```
If you'd like to know the full details of how word embedding works, as well as a better understanding of how we create our visualizations, please look [here](https://nbviewer.jupyter.org/github/PhilChodrow/PIC16B/blob/master/lectures/tf/tf-3.ipynb#Embeddings) at Professor Phil Chodrow's notes on it. The below will be specific to our objective at hand rather than what he does in his lecture.
```

</div>
{::options parse_block_html="false" /}

We will now visualize and discuss the `embedding` that our model learned to see if we can find any interesting patterns or association in the words that the model found useful when distinguishing real and fake news. If you'd like to know the full details of how word embedding works, as well as a better understanding of how we create our visualizations, please look [here](https://nbviewer.jupyter.org/github/PhilChodrow/PIC16B/blob/master/lectures/tf/tf-3.ipynb#Embeddings) at Professor Phil Chodrow's notes on it. The below will be specific to our objective at hand rather than what he does in his lecture.

We will first display a summary of our `text_model`, specifically to obtain the name of our `Embedding` layer:



```python
text_model.summary() # To see our Embedding layer label
```
    Model: "model_1"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    text (InputLayer)            [(None, 1)]               0         
    _________________________________________________________________
    text_vectorization_1 (TextVe (None, 500)               0         
    _________________________________________________________________
    embedding_1 (Embedding)      (None, 500, 60)           120000    
    _________________________________________________________________
    global_average_pooling1d_1 ( (None, 60)                0         
    _________________________________________________________________
    dense_1 (Dense)              (None, 64)                3904      
    _________________________________________________________________
    fake (Dense)                 (None, 1)                 65        
    =================================================================
    Total params: 123,969
    Trainable params: 123,969
    Non-trainable params: 0
    _________________________________________________________________
    
```python
weights = text_model.get_layer('embedding_1').get_weights()[0] # get the weights from the embedding layer
vocab = vectorize_text.get_vocabulary()                        # get the vocabulary from our data prep for later

weights, weights.shape
```
    (array([[ 0.03695452, -0.00117357, -0.04044463, ...,  0.00556057,
             -0.03298749, -0.02513593],
            [-0.00062775, -0.06685973, -0.00085732, ..., -0.01607611,
              0.02754348,  0.04649191],
            [-0.45944422,  0.4769331 ,  0.44603348, ..., -0.5297867 ,
              0.54784036,  0.44726244],
            ...,
            [-0.2511302 ,  0.25237024,  0.2451331 , ..., -0.27041462,
              0.21960059,  0.2123137 ],
            [ 0.10782911, -0.04370306, -0.09748171, ...,  0.06245572,
             -0.0782387 , -0.10043219],
            [ 0.263783  , -0.24967179, -0.23989156, ...,  0.23634155,
             -0.2277107 , -0.27298978]], dtype=float32), (2000, 60))

We can see that the collection of weights is 60-dimensional by way of our output dimension, and now utilize PCA (principal component analysis) to reduce our dimensions to 2D representation, to which we'll make a data from our results with `pandas`.

```python
embedding_df = pd.DataFrame({
    'word' : vocab, 
    'x0'   : weights[:,0],
    'x1'   : weights[:,1]
})
embedding_df
```

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>word</th>
      <th>x0</th>
      <th>x1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td></td>
      <td>0.036955</td>
      <td>-0.001174</td>
    </tr>
    <tr>
      <th>1</th>
      <td>[UNK]</td>
      <td>-0.000628</td>
      <td>-0.066860</td>
    </tr>
    <tr>
      <th>2</th>
      <td>said</td>
      <td>-0.459444</td>
      <td>0.476933</td>
    </tr>
    <tr>
      <th>3</th>
      <td>trump</td>
      <td>0.125928</td>
      <td>-0.216383</td>
    </tr>
    <tr>
      <th>4</th>
      <td>the</td>
      <td>-0.267981</td>
      <td>0.286431</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>1995</th>
      <td>footage</td>
      <td>-0.162400</td>
      <td>0.120575</td>
    </tr>
    <tr>
      <th>1996</th>
      <td>controversy</td>
      <td>0.127674</td>
      <td>-0.217168</td>
    </tr>
    <tr>
      <th>1997</th>
      <td>challenges</td>
      <td>-0.251130</td>
      <td>0.252370</td>
    </tr>
    <tr>
      <th>1998</th>
      <td>unclear</td>
      <td>0.107829</td>
      <td>-0.043703</td>
    </tr>
    <tr>
      <th>1999</th>
      <td>stupid</td>
      <td>0.263783</td>
      <td>-0.249672</td>
    </tr>
  </tbody>
</table>
<p>2000 rows × 3 columns</p>
</div>

Now we are ready to plot our embedding, almost linear looking as we have two categories, fake news or not fake news.

```python
import plotly.express as px 
fig = px.scatter(embedding_df, 
                 x = "x0", 
                 y = "x1", 
                 size = list(np.ones(len(embedding_df))),
                 size_max = 2,
                 hover_name = "word")

fig.show()
```
{% include embed_visual.html %}

We can see we have "Trumps", "Obamas", and "Mugabe" as far top left together (who are/were world leaders), whereas on the bottom right we see "rep", "GOP", and "press" together, maybe regarding press conferences and representatives, and so on.

